---
title: "Exercise Evaluation"
author: "Stephen Won"
date: "06/21/2014"
output: html_document
---

When performing exercise, it is important to have the correct form and movement 
to avoid injury and maximize efficiency. When given the proper equipment, we can
diagnose when someone is doing exervise correctly using existing, machine
learning libraries in R. We take a look how this is possible by using the Human 
Activity Recognition dataset [1].

## Prerequisite Conditions

Since we are doing machine learning experiments, we will need the **caret** 
library since it contains all of the methods that we need. We should also set 
the seed for random numbers in order to allow experiments to be repeatable. 

To accelerate computations, we should aim to perform operations in parallel as 
much as possible. To that end, I import the **doMC** library since most of the 
caret functions can take advantage of the multiple cores. The following setup 
is performed: 

```{r}
library(caret)
library(doMC)
registerDoMC(cores = 8)
set.seed(6348)
```

## Preprocessing the Data

As with any machine learning project, we should first take a look at the data. 
A significant amount of columns are blank or not available. To simplify our 
model, we remove any column possessing either of these two traits. Afterwards, 
we should notice that the first eight columns are measurements not pertaining 
to the sensor. We manually remove them before running PCA on the remaining data 
columns.

In order to get an idea of how well our algorithm will perform on the testing 
data, we create a split to make a "cross-validated" data partition. This is an 
80/20 split in order to give sufficient data to developing the models while 
giving a good estimate to testing performance. Later on, we'll let caret train 
using a 4-fold cross-validation setup.

In order to capture most of the variance of the data and to remove repeated 
operations, we pre-process the data with PCA outside of caret's train function. 
We also center and scale the data in order to treat each column equally. The 
threshold for the PCA was left at the defaul 0.95 in order to capture most of 
the variance. Finally, we use the predict function to apply our PCA to the data.

The preprocessing is performed to both the training and testing data like so:

```{r}
# Read in the data
raw_train_data = read.csv("pml-training.csv", header = TRUE, 
                          na.strings=c("", "NA"))
raw_test_data = read.csv("pml-testing.csv", header = TRUE, 
                          na.strings=c("", "NA"))

# Remove the NA/empty columns
raw_train_data = raw_train_data[, colSums(is.na(raw_train_data)) == 0]
raw_test_data = raw_test_data[, colSums(is.na(raw_test_data)) == 0]

# Remove non-sensor measurements
raw_train_data = raw_train_data[, 8:length(raw_train_data)]
raw_test_data = raw_test_data[, 8:length(raw_test_data)]

# Divide the training data into 80/20 training/CV and remove classe for PCA
train_data = raw_train_data[, 1:length(raw_train_data) - 1]
test_data = raw_test_data[,1:length(raw_test_data) - 1]

# Define the PCA operation
pre_proc = preProcess(rbind(train_data, test_data), 
                      method = c("center", "scale", "pca"))

# Apply the PCA to the data
in_train = createDataPartition(y = raw_train_data$classe, p = 0.8, list = FALSE)
pca_data = predict(pre_proc, train_data)
pca_train_data = pca_data[in_train,]
pca_cv_data = pca_data[-in_train,]
pca_test_data = predict(pre_proc, test_data)

# Create classe variables for easier access
train_classe = raw_train_data[in_train, length(raw_train_data)]
cv_classe = raw_train_data[-in_train, length(raw_train_data)]
```

## Data Modeling

Once all of the data has been preprocessed, we can begin to model the data to 
the classe. In the Netflix contest, the top performers used an ensemble of 
models in their predictions. To model this data, I decided to adopt the same 
idea and use multiple models to determine whether or not someone is performing 
the exercise correctly. 

I decided to use the following four models: recursive partitioning and 
regression trees (**rpart**), random forests (**rf**), linear discriminant 
analysis (**lda**), and radial support vector machines (**svmRadial**). All of 
them were picked arbitrarily. Other models such as the generalized boosted 
regression models (**gbm**) were not selected on the basis of time and 
processing constraints. 

To ensure that we do not overfit on the training data, we perform 
cross-validation while training our models. Since we already separated 20% of 
our training data to get a performance estimate, I decided to perform a 4-fold 
cross-validation so that the training data and cross-validated partition within 
the train function is 60% and 20%, respectively, for each of the repeats. 

The model training is performed like so:

```{r training_model, cache = TRUE}
# Define k-fold cross-validation training conditions
fit_control = trainControl(method = "repeatedcv", number = 4, repeats = 4)

# Train multiple models under default parameters
rpart_model = train(pca_train_data, train_classe, 
                    method = "rpart", trControl = fit_control)
rf_model = train(pca_train_data, train_classe,
                 method = "rf", trControl = fit_control)
lda_model = train(pca_train_data, train_classe, 
                  method = "lda", trControl = fit_control)
svm_model = train(pca_train_data, train_classe, 
                  method = "svmRadial", trControl = fit_control)
```

After training the models, we could take a look at the in-sample error for each 
of the individual models. However, we can instead just combine all of them to 
form an ensemble. To do this, we predict on the training set for each of the 
models and create a new data frame containing the predictions and the classe. 
We then fit a random forest over the predictions to the classe, thus forming an 
ensemble prediction as performed here:

```{r ensemble_model, cache = TRUE}
# Predict with each of the trained models
rpart_train_pred = predict(rpart_model, pca_train_data)
rf_train_pred = predict(rf_model, pca_train_data)
lda_train_pred = predict(lda_model, pca_train_data)
svm_train_pred = predict(svm_model, pca_train_data)

# Fit a model combining the predictions
train_df = data.frame(rpart_pred = rpart_train_pred, rf_pred = rf_train_pred, 
                      lda_pred = lda_train_pred, svm_pred = svm_train_pred, 
                      classe = train_classe)
ensemble_model = train(classe ~ ., method = "rf", data = train_df)
print(ensemble_model)
```

We can get the in-sample error for our ensemble by subtracting with our highest 
accuracy attained with our model. The in-sample error is:

```{r}
error = 1 - max(ensemble_model$results$Accuracy)
names(error) = "Error"
print(error)
```

## Prediction with Trained Models

To get the out-of-sample error, we use the ensemble model on the
cross-validation data partition that was not included in the training. We run 
the data through each of the individual models before using the ensemble model.

```{r}
# Generate predictions on the data
rpart_cv_pred = predict(rpart_model, pca_cv_data)
rf_cv_pred = predict(rf_model, pca_cv_data)
lda_cv_pred = predict(lda_model, pca_cv_data)
svm_cv_pred = predict(svm_model, pca_cv_data)

# Fit a model combining the predictions
cv_df = data.frame(rpart_pred = rpart_cv_pred, rf_pred = rf_cv_pred, 
                   lda_pred = lda_cv_pred, svm_pred = svm_cv_pred, 
                   classe = cv_classe)
cv_pred = predict(ensemble_model, cv_df)
```

By using a confusion matrix, we can get the accuracy of ensemble model on the 
cross-validation data partition. The expected out-of-sample error is:

```{r}
cv_confusion_matrix = confusionMatrix(cv_pred, cv_classe)
error = 1 - cv_confusion_matrix$overall["Accuracy"]
names(error) = "Error"
print(error)
```

## Predicting on the Test Set

Similar to the cross-validation data partition, we can predict on the test data. 
Since the test data does not have the classe, this section is just to output the 
results submitted to Coursera.

```{r}
# Generate predictions on the data
rpart_test_pred = predict(rpart_model, pca_test_data)
rf_test_pred = predict(rf_model, pca_test_data)
lda_test_pred = predict(lda_model, pca_test_data)
svm_test_pred = predict(svm_model, pca_test_data)

# Fit a model combining the predictions
test_df = data.frame(rpart_pred = rpart_test_pred, rf_pred = rf_test_pred, 
                     lda_pred = lda_test_pred, svm_pred = svm_test_pred)
test_pred = predict(ensemble_model, test_df)

# Write out predictions to file. Code taken from project submission page.
n = length(test_pred)
for(i in 1:n){
  filename = paste0("problem_id_", i, ".txt")
  write.table(test_pred[i], file = filename, quote = FALSE, 
              row.names = FALSE, col.names = FALSE)
}
```


#### Citations
1) Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.